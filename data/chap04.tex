\chapter{实验验证与分析}
\label{cha:exp}
在实验中，本文在COCO2017验证数据集上达成了46.7mAP的姿态检测结果，证明了本文中的优化模块的确优化了多人姿态估计网络的性能。同时，为了证明方法的有效性，本文完成了一些实验来证明本文提出的弱监督下的注意力以及其对应的网络结构能够较好地在解决多人遮挡的问题的同时，改善关键点与实例分割的结果。
\section{数据集与评价指标}
\label{sec:dataset}
COCO2017数据集包含了15万个人体以及将近170万个标注的关键点信息。本文在使用该数据集时，将数据集中的标注数据进行了清洗。本文将小于3个关键点的数据过滤，从而在训练数据集中选出了满足要求的二十六万个训练数据以及用于验证的一万一千个验证数据。每个数据由关键点信息以及目标框组成。本文中为了整体训练整个网络，同时使用了数据集中的关键点以及目标框的信息。

为了验证在\ref{sec:refine}节中提出的网络结构，本文设计了使用mAP作为评价指标的消融实验与性能对比实验。在实验中本文使用了COCO2017验证数据集\cite{lin2014microsoft}作为本文验证的数据集。平均准确度(mAP，mean Average Precision)\cite{zhu2004recall}指标都是旨在统计一定容忍度下的关键点命中情况。平均准确度通过计算交并比(IoU, Intersection over Union)来求得不同物体关键点相似度(OKS，Object Key-point Similarity)\footnote{物体关键点相似度从0.5取值至0.95，间隔为0.01。}下的准确率，并对这些准确率进行积分得到最终平均准确率。

\begin{figure}
	\centering
	\includegraphics[width=0.6\linewidth]{OKS.png}
	\caption{物体关键点相似度图示\cite{ruggero2017benchmarking}}
	\label{fig:oksfigure}
\end{figure}

物体关键点在计算交并比的时候，需要设置关键点的容许度半径，如图\ref{fig:oksfigure}中所示。不同的关键点拥有不同的容许度半径。比如眼睛的容许度半径一定比手腕的容许度范围要小。在图\ref{fig:oksfigure}中，手腕与眼睛的预测关键点与真值的距离相当，然而给出的命中判定却不相同。因为人类对眼睛关键点的标注方差更小，对手腕关键点的标注方差更大。COCO数据集根据人类对于不同关键点标注的方差大小设置容许度半径来判别关键点命中是合理的。这比一些通过标准化距离阈值来判别命中的方法，比如关键点命中概率曲线\cite{andriluka20142d}(PCK，Probability of Correct Key-point)更加科学。

\section{性能对比}
\label{sec:perfcompare}
本节中文章主要通过对比网络在mAP指标上的表现，来对比网络与现有方法的性能。在对比的同时还会给出结果分析，尝试从实验结果验证网络结构设计的优势。本文选取了自顶向下结构的堆叠沙漏模型作为本文的对比方法。
% TODO: 比较表格
\begin{table*}[ht]
	\centering
	\caption{COCO公开测试集的模型性能对比}
	\label{tab:mAPCOCObenchmark}
	\begin{minipage}[t]{0.8\linewidth}
		\begin{tabular}{p{0.25\linewidth}p{0.1\linewidth}<{\centering}p{0.1\linewidth}<{\centering}p{0.1\linewidth}<{\centering}p{0.1\linewidth}<{\centering}p{0.1\linewidth}<{\centering}}
			\hline
			方法 & \multicolumn{1}{c}{$mAP$} & \multicolumn{1}{c}{$AP_{OKS=0.5}$} & \multicolumn{1}{c}{$AP_{OKS=0.75}$}
			& \multicolumn{1}{c}{$AP_M$} & \multicolumn{1}{c}{$AP_L$} \\
			
			& \multicolumn{1}{c}{(\%)}& \multicolumn{1}{c}{(\%)}&
			\multicolumn{1}{c}{(\%)}& \multicolumn{1}{c}{(\%)}& \multicolumn{1}{c}{
				(\%)}\\
			\hline
			Top-down SHN\cite{newell2016stacked} & 46.0 & 74.6 & \textbf{48.4} & 38.8  & \textbf{55.6} \\
			本文复现 & 41.2 & 71.3 & 45.9 & 34.2 & 47.4 \\
			本文方法$^1$ & 45.9 & 82.1 & 45.3 & 43.0 & 51.6 \\
			本文方法+ $^2$ & \textbf{46.7} & \textbf{83.4} & 46.6 & \textbf{45.1} & 53.2 \\
			\hline
		\end{tabular}\\[2pt]
		\noindent\rule{0.25\linewidth}{1pt} \\
		\footnotesize
		1: 使用直接连接的4个堆叠的优化模块，在本文提出的整体训练策略下收敛的模型。\\
		2: 为了增加网络感受野而使用后置的类似基准方法中出现的优化模块，在本文提出的整体训练策略下收敛的模型。
	\end{minipage}
\end{table*}

首先需要关注复现的基准方法相对于本文提出的方法的性能提升。很明显这两种使用优化模块的策略均比基准方法更优或表现相当。对于本文方法$^1$，除在$AP_{OKS=0.75}$的指标下比复现方法略低以外，该策略下的其他指标数值均优于复现的基准方法。而之后在此基础上额外增加后置类似基准方法中出现的优化模块的本文方法+$^2$同样在原有的基础上提高了$0.8mAP$，但是在$AP_{OKS=0.75}$的指标上虽然超过了复现方法，但仍低于用于对比的其他方法。对比方法虽然在$AP_{OKS=0.75}$与$AP_L$指标上略高于本文方法，但是本文在$AP_{OKS=0.5}$指标上远远高于对比方法。整体而言，本文方法比现有的自顶向下的堆叠沙漏网络在性能上高出$0.7mAP$。

相对于在表\ref{tab:mAPCOCObenchmark}中，可以明显看到本文在宽松条件下（OKS=0.5）的关键点准确度明显优于本文复现的自顶向下框架和SHN网络的结果。这一结果要归功于网络中使用弱监督学习方法监督的注意力机制，其生成的关注区域帮助网络生成更加紧凑的关键点结果。同时在小目标检测\footnote{在COCO数据集中，小目标通常指边界框面积小于$64^2$的目标。}中明显优于传统的自顶向下的方法。大目标中出现的性能损失主要是来源于直接的特征剪裁方法。在其他自顶向下方法中，普遍采取的特征提取方法是将原图像根据检测框裁剪，再送入姿态估计网络。本结构中使用的剪裁策略是直接使用特征进行剪裁。这会导致较大尺度的特征信息损失更多，因此可能会造成在更大尺寸图像上的姿态估计结果精度下降的问题。本文方法在高精度要求的指标下略微低于对比方法。这可能是由于损失函数相互竞争而造成的后果。本文通过在损失函数中设定合适的$\alpha$值来确保竞争尽可能小。同时，整体层面上，本文方法仍然要由于基线方法与堆叠沙漏网络。

\section{消融实验与分析}
\label{sec:ablation}
本节中文章设计了消融实验验证优化模块的作用以及有效性。本文提出了多种消融策略来控制优化模块中不同部件出现，从而通过量化指标和可视化效果来量化优化模块对提高算法性能的贡献。
\subsection{消融策略}
\label{subsec:selfstrategy}
由于本文提出了一种能够融合实例分割信息的新结构，因此需要设计自对比实验来证明文章提出的结构能够改善基线方法。本文中除了多任务融合+策略使用了6个堆叠的优化模块\footnote{由于过多的优化模块不能帮助优化模块细化注意力(如图\ref{fig:multistageattention})，该策略中最后两个优化模块没有使用注意力机制融合两分支。}以外，其余所有方法都是用了4个堆叠的优化模块。本文通过摘除优化模块中对应的网络结构来完成用于模块验证的消融实验。
本文设计了三种自对比的网络结构策略：
\begin{itemize}
	\item \textbf{单任务无融合}：与复现基准方法相同，不使用图\ref{fig:RefineNet}中对应的注意力转换模块、分割子网，仅留下优化模块中的共享卷积层与姿态子网的简单基准网络结构。
	\item \textbf{多任务无融合}：除单任务无融合网络结构中描述的策略以外，额外使用图\ref{fig:RefineNet}中的分割子网，但不使用注意力转换模块以及两个卷积门将生成的注意力融合进入两分支的多任务网络结构。
	\item \textbf{多任务融合}：使用图\ref{fig:RefineNet}中所有的卷积层，并使用弱监督下的空间注意力融合两分支特征的网络结构。
	\item \textbf{多任务融合+}：在多任务融合网络结构的基础上，加入后置的类似单任务无融合中出现的优化模块来扩大网络感受野的网络结构策略。
\end{itemize}

\subsection{消融性能实验}
\label{subsec:selfeval}

本文根据\ref{subsec:selfstrategy}节中提到的消融策略完成了在COCO数据集上的消融性能实验。实验结果如表\ref{tab:mAPCOCOselfbenchmark}所示。

% TODO: 自对比表格数据填充
\begin{table}[ht]
	\centering
	\caption{自对比mAP评价数据}
	\label{tab:mAPCOCOselfbenchmark}
	\begin{minipage}{0.8\linewidth}
		\begin{tabular}{p{0.25\linewidth}p{0.1\linewidth}<{\centering}p{0.1\linewidth}<{\centering}p{0.1\linewidth}<{\centering}p{0.1\linewidth}<{\centering}p{0.1\linewidth}<{\centering}}
			\hline
			方法 & \multicolumn{1}{c}{$mAP$} & \multicolumn{1}{c}{$AP_{OKS=0.5}$} & \multicolumn{1}{c}{$AP_{OKS=0.75}$} 
			& \multicolumn{1}{c}{$AP_M$} & \multicolumn{1}{c}{$AP_L$} \\
			
			& \multicolumn{1}{c}{(\%)}& \multicolumn{1}{c}{(\%)}&
			\multicolumn{1}{c}{(\%)}& \multicolumn{1}{c}{(\%)}& \multicolumn{1}{c}{
				(\%)}\\
			\hline
			单任务无融合 & 41.2 & 71.3 & 45.9& 34.2& 47.4\\
			多任务无融合 & 38.8 & 78.4 & 25.6 & 30.1 & 37.7 \\
			多任务有融合 & 45.9 & 82.1 & 45.3 & 43.0 & 51.6 \\
			多任务有融合+ & \textbf{46.7} & \textbf{83.4} & \textbf{46.6} & \textbf{45.1} & \textbf{53.2} \\
			\hline
		\end{tabular}
	\end{minipage}
\end{table}

表格\ref{tab:mAPCOCOselfbenchmark}描述了本方法在COCO开放验证数据集上的表现情况。其中，单任务无融合策略是这四种策略中的基准，因为其仅仅完成单个关键点定位任务，且没有引入注意力机制。在表\ref{tab:mAPCOCOselfbenchmark}中可以看到，多任务无融合的实验结果明显低于本文的基准方法，也就意味着简单将多任务直接使用多分支回归会影响最终关键点定位的精度。可以明显地看到在多任务无融合的策略下，在高精度阈值下的准确率明显下降了。

然而在加入融合，也就是使用空间注意力将两任务连接后，网络在高精度要求下的准确度大幅提升。这是得益于多任务的损失函数可以通过梯度回传至分散的分支以让网络更好地收敛。相比之下，网络能够更好地融合实例分割，让关键点检测的结果比基准方法中给出的有多提高。这证明了使用注意力连接的优化模块比一般的多任务分支网络性能能更好，同时也验证了使用优化模块来加入实例分割信息能够有效改善关键点行为的性能。

\subsection{注意力可视化}
\label{sec:weaksuperatten}
本文将网络中输出的空间注意力可视化，并于原图混合得到了最终可视化的结果。由于空间注意力是一个单通道的热图，因此本文使用了Plasma方式映射颜色空间，得到了在图\ref{fig:attenvis}右侧的可视化结果。如图\ref{fig:attenvis}中所示，注意力能够较为明确地给出遮挡人轮廓边界。同时对于被遮挡人不可见身体部分的对应区域，空间注意力也能给出一定的响应。

\begin{figure}[h]
	\centering
	\begin{subfigure}{0.3\linewidth}
		\includegraphics[width=\linewidth]{60507.png}
		\caption{原始图像}
	\end{subfigure}
	\begin{subfigure}{0.3\textwidth}
		\centering
		\begin{subfigure}{\linewidth}
			\includegraphics[width=\linewidth]{60507_insid1_stage1_atten.png}
		\end{subfigure}
		\vskip2pt
		\begin{subfigure}{\linewidth}
			\includegraphics[width=\linewidth]{60507_insid1_stage2_atten.png}
		\end{subfigure}
		\vskip2pt
		\begin{subfigure}{\linewidth}
			\includegraphics[width=\linewidth]{60507_insid1_stage3_atten.png}
		\end{subfigure}
		\caption{遮挡人}
	\end{subfigure}
	\begin{subfigure}{0.3\textwidth}
		\centering
		\begin{subfigure}{\linewidth}
			\includegraphics[width=\linewidth]{60507_insid12_stage1_atten.png}
		\end{subfigure}
		\vskip2pt
		\begin{subfigure}{\linewidth}
			\includegraphics[width=\linewidth]{60507_insid12_stage2_atten.png}
		\end{subfigure}
		\vskip2pt
		\begin{subfigure}{\linewidth}
			\includegraphics[width=\linewidth]{60507_insid12_stage3_atten.png}
		\end{subfigure}
		\caption{被遮挡人}
	\end{subfigure}
	\begin{minipage}{0.05\linewidth}
		\rotatebox[origin=c]{90}{\centering 阶段2}
		
		\vskip2cm
		\rotatebox[origin=c]{90}{\centering 阶段3}
		
		\vskip2cm
		\rotatebox[origin=c]{90}{\centering 阶段4}
	\end{minipage}
	\caption{在$\alpha=0.1$遮挡情况下的空间注意力可视化结果}
	\label{fig:attenvis}
\end{figure}

在全局损失函数(公式\eqref{detection_loss})中不同的$\alpha$会影响注意力的生成。根据\ref{subsec:gradient}节中的推导，更低的$\alpha$会引导注意力在训练过程中缺少来自实例分割的引导。这会让注意力关注的区域更加泛化，难以引导关键点分支关注正确的区域。本文使用不同的$\alpha$取值训练网络，同来对比生成的注意力区域的精细程度。由于网络生成的注意力对关键点任务十分重要，因此注意力生成的质量可以间接表明模型的性能。


\begin{figure}[h]
	\centering
	\begin{subfigure}{0.3\textwidth}
		\centering
		\begin{subfigure}{\linewidth}
			\includegraphics[width=\linewidth]{60507_insid1_stage3_atten.png}
		\end{subfigure}
		\vskip2pt
		\begin{subfigure}{\linewidth}
			\includegraphics[width=\linewidth]{60507_insid12_stage3_atten.png}
		\end{subfigure}
		\caption{$\alpha=0.1$}
	\end{subfigure}
	\begin{subfigure}{0.3\textwidth}
		\centering
		\begin{subfigure}{\linewidth}
			\includegraphics[width=\linewidth]{60507_insid1_stage1_atten_alpha.png}
		\end{subfigure}
		\vskip2pt
		\begin{subfigure}{\linewidth}
			\includegraphics[width=\linewidth]{60507_insid12_stage1_atten_alpha.png}
		\end{subfigure}
		\caption{$\alpha=0.01$}
	\end{subfigure}
	\begin{minipage}{0.05\linewidth}
		\rotatebox[origin=c]{90}{\centering 遮挡人}
		
		\vskip2cm
		\rotatebox[origin=c]{90}{\centering 被遮挡人}
	\end{minipage}
	\caption{在$\alpha$取值下训练得到的遮挡情况下的空间注意力可视化结果}
	\label{fig:attenalpha}
\end{figure}

正如上文所推测，更小的$\alpha$取值非但没有让注意力更多地关注关键点区域，反而让注意力生成更加偏离实例分割的结果。这样与本文希望注意力能够达到的目标相悖，因而在实验中选取了$\alpha=0.1$作为损失函数中平衡实例分割与关键点影响的因子来训练网络。

同时，通过实验发现(如图\ref{fig:failcase})，在注意力无法区分单个人体区域的时候，会比较明显地影响关键点定位结果。对于注意力而言，在过多遮挡的场景中辨析单个人体是一个较为困难的任务。由于缺乏足够的关键点与实例分割检测结果，自生成的空间注意力会无法将两人分开。这也是现有方法中普遍存在的问题。但在轻度遮挡的情况下，本文方法仍拥有区分出不同人体区域的能力。

\begin{figure}[h]
	\centering
	\begin{subfigure}[t]{0.22\textwidth}
		\centering
		\begin{subfigure}[t]{\linewidth}
			\includegraphics[width=\linewidth]{872.png}
		\end{subfigure}
		\vskip2pt
		\begin{subfigure}[t]{\linewidth}
			\includegraphics[width=\linewidth]{872_insid-1_stage3_keypoint.png}
			\caption{输入图像与姿态}
		\end{subfigure}
	\end{subfigure}
	\begin{subfigure}[t]{0.22\textwidth}
		\centering
		\begin{subfigure}[t]{\linewidth}
			\includegraphics[width=\linewidth]{872_insid1_stage1_atten.png}
		\end{subfigure}
		\vskip2pt
		\begin{subfigure}[t]{\linewidth}
			\includegraphics[width=\linewidth]{872_insid0_stage1_atten.png}
			\caption{阶段2}
		\end{subfigure}
	\end{subfigure}
	\begin{subfigure}[t]{0.22\textwidth}
		\centering
		\begin{subfigure}[t]{\linewidth}
			\includegraphics[width=\linewidth]{872_insid1_stage2_atten.png}
		\end{subfigure}
		\vskip2pt
		\begin{subfigure}[t]{\linewidth}
			\includegraphics[width=\linewidth]{872_insid0_stage2_atten.png}
			\caption{阶段3}
		\end{subfigure}
	\end{subfigure}
	\begin{subfigure}[t]{0.22\textwidth}
		\centering
		\begin{subfigure}[t]{\linewidth}
			\includegraphics[width=\linewidth]{872_insid1_stage3_atten.png}
		\end{subfigure}
		\vskip2pt
		\begin{subfigure}[t]{\linewidth}
			\includegraphics[width=\linewidth]{872_insid0_stage3_atten.png}
			\caption{阶段4}
		\end{subfigure}
	\end{subfigure}
	\begin{minipage}{0.05\linewidth}
		\rotatebox[origin=c]{90}{\centering 遮挡人}
		
		\vskip2cm
		\rotatebox[origin=c]{90}{\centering 被遮挡人}
	\end{minipage}
	\caption{极端遮挡场景下的注意力可视化结果}
	\label{fig:failcase}
\end{figure}



\section{视觉效果与分析}
\label{sec:demo}

\begin{figure}[htbp]
	\centering
	\begin{minipage}[t]{\linewidth}
		\centering
		\begin{subfigure}[b]{0.23\linewidth}
			\includegraphics[width=\linewidth]{885.png}
		\end{subfigure}
		\begin{subfigure}[b]{0.23\linewidth}
			\includegraphics[width=\linewidth]{885_insid-1_stage3_atten.png}
		\end{subfigure}
		\begin{subfigure}[b]{0.23\linewidth}
			\includegraphics[width=\linewidth]{885_insid-1_mask.png}
		\end{subfigure}
		\begin{subfigure}[b]{0.23\linewidth}
			\includegraphics[width=\linewidth]{885_insid-1_stage5_keypoint.png}
		\end{subfigure}
		
		\vskip5pt
		\begin{subfigure}[b]{0.23\linewidth}
			\includegraphics[width=\linewidth]{1000.png}
		\end{subfigure}
		\begin{subfigure}[b]{0.23\linewidth}
			\includegraphics[width=\linewidth]{1000_insid-1_stage3_atten.png}
		\end{subfigure}
		\begin{subfigure}[b]{0.23\linewidth}
			\includegraphics[width=\linewidth]{1000_insid-1_mask.png}
		\end{subfigure}
		\begin{subfigure}[b]{0.23\linewidth}
			\includegraphics[width=\linewidth]{1000_insid-1_stage5_keypoint.png}
		\end{subfigure}
		
		\vskip5pt
		\begin{subfigure}[b]{0.23\linewidth}
			\includegraphics[width=\linewidth]{13729.png}
		\end{subfigure}
		\begin{subfigure}[b]{0.23\linewidth}
			\includegraphics[width=\linewidth]{13729_insid-1_stage3_atten.png}
		\end{subfigure}
		\begin{subfigure}[b]{0.23\linewidth}
			\includegraphics[width=\linewidth]{13729_insid-1_mask.png}
		\end{subfigure}
		\begin{subfigure}[b]{0.23\linewidth}
			\includegraphics[width=\linewidth]{13729_insid-1_stage5_keypoint.png}
		\end{subfigure}
		
		\vskip5pt
		\begin{subfigure}[b]{0.23\linewidth}
			\includegraphics[width=\linewidth]{18491.png}
		\end{subfigure}
		\begin{subfigure}[b]{0.23\linewidth}
			\includegraphics[width=\linewidth]{18491_insid-1_stage3_atten.png}
		\end{subfigure}
		\begin{subfigure}[b]{0.23\linewidth}
			\includegraphics[width=\linewidth]{18491_insid-1_mask.png}
		\end{subfigure}
		\begin{subfigure}[b]{0.23\linewidth}
			\includegraphics[width=\linewidth]{18491_insid-1_stage5_keypoint.png}
		\end{subfigure}
		
		\vskip5pt
		\begin{subfigure}[b]{0.23\linewidth}
			\includegraphics[width=\linewidth]{38829.png}
		\end{subfigure}
		\begin{subfigure}[b]{0.23\linewidth}
			\includegraphics[width=\linewidth]{38829_insid-1_stage3_atten.png}
		\end{subfigure}
		\begin{subfigure}[b]{0.23\linewidth}
			\includegraphics[width=\linewidth]{38829_insid-1_mask.png}
		\end{subfigure}
		\begin{subfigure}[b]{0.23\linewidth}
			\includegraphics[width=\linewidth]{38829_insid-1_stage5_keypoint.png}
		\end{subfigure}
		
		\vskip5pt
		\begin{subfigure}[b]{0.23\linewidth}
			\includegraphics[width=\linewidth]{53626.png}
		\end{subfigure}
		\begin{subfigure}[b]{0.23\linewidth}
			\includegraphics[width=\linewidth]{53626_insid-1_stage3_atten.png}
		\end{subfigure}
		\begin{subfigure}[b]{0.23\linewidth}
			\includegraphics[width=\linewidth]{53626_insid-1_mask.png}
		\end{subfigure}
		\begin{subfigure}[b]{0.23\linewidth}
			\includegraphics[width=\linewidth]{53626_insid-1_stage5_keypoint.png}
		\end{subfigure}
		
		\vskip5pt
		\begin{subfigure}[b]{0.23\linewidth}
			\includegraphics[width=\linewidth]{68387.png}
			\caption{输入图像}
		\end{subfigure}
		\begin{subfigure}[b]{0.23\linewidth}
			\includegraphics[width=\linewidth]{68387_insid-1_stage3_atten.png}
			\caption{空间注意力}
		\end{subfigure}
		\begin{subfigure}[b]{0.23\linewidth}
			\includegraphics[width=\linewidth]{68387_insid-1_mask.png}
			\caption{实例分割}
		\end{subfigure}
		\begin{subfigure}[b]{0.23\linewidth}
			\includegraphics[width=\linewidth]{68387_insid-1_stage5_keypoint.png}
			\caption{关键点检测}
		\end{subfigure}
	\end{minipage}
	\caption{本文方法得到的可视化对比图}
	\label{fig:megavis}
\end{figure}

本文通过大量的实验定量与定性实验证明了文章提出的网络结构以及训练方式能够有效地融合实例分割与姿态估计的信息，并通过弱监督学习训练的空间注意力改善这两个任务中的特征融合以及特征表达。本文提出的优化模块能够同时给出较好地关键点与分割的检测结果，并能够一定程度上地改善多人场景下的姿态遮挡问题。

本文将部分可视化效果在图\ref{fig:megavis}中展示。可以看到本文方法在多人场景中达到了预期的效果。网络同时通过注意力机制学习关注了不同人体之间的关系，并且根据该线索预测出被遮挡区域的关键点。



\section{本章小结}
本节中，文章在COCO2017数据集上对比了现有方法与本文方法的性能指标，并通过设计多个对比消融实验分别证明了本文中提出的优化模块以及损失函数平衡策略的有效性。通过大量图例以及可视化结果，展示了网络模型同时给出分割结果和姿态估计结果的性能。